{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "# print(os.path.realpath(__file__))\n",
    "sys.path.append(str(Path(sys.argv[0]).absolute().parent.parent.parent.parent))\n",
    "base_repo = os.path.realpath(os.path.join(os.getcwd(), \"../../\"))\n",
    "print(base_repo)\n",
    "sys.path.append(base_repo)\n",
    "# add the entire folder to path\n",
    "print(sys.path)\n",
    "print(os.getcwd())\n",
    "from src.discovery import data\n",
    "from src.discovery import classifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_DIR = os.path.realpath('./output/')\n",
    "PATH_TO_CSV = os.path.realpath('../data/raw/2022-10-26_hiscore_data.csv')\n",
    "if PATH_TO_CSV is None or not os.path.exists(PATH_TO_CSV):\n",
    "    print('set PATH_TO_CSV=/path/to/2022-10-26_hiscore_data.csv and run all')\n",
    "    raise ValueError\n",
    "\n",
    "df = pd.read_csv(PATH_TO_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This logic is roughly equivalent to app.py's train function\n",
    "hiscoredata = data.hiscoreData(df, False)\n",
    "features = hiscoredata.features()\n",
    "# no playerData()-related data, its already in features\n",
    "features_labeled = features\n",
    "binary_classifier = classifier.classifier(\"binaryClassifier\")\n",
    "# Logic should be the same as app.py line 179: train the model \n",
    "x = features_labeled.copy()\n",
    "y = x['label'].apply(lambda x: 0 if x == 'Real_Player' else 1)\n",
    "# TODO: one-hot encode account status\n",
    "print('unique account_status values:', x['account_status'].unique())\n",
    "# TODO: using created_at, updated_at epoch times slightly improve binary classifier\n",
    "# x['created_at'] = pd.to_datetime(x['created_at'], format=\"%Y-%m-%d %H:%M:%S\").apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(\"1s\"))\n",
    "# x['updated_at'] = pd.to_datetime(x['updated_at'], format=\"%Y-%m-%d %H:%M:%S\").apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(\"1s\"))\n",
    "# print(x['created_at'].head())\n",
    "x.drop(columns=['label', 'label_id', 'name', 'created_at', 'updated_at', 'account_status', 'possible_ban', 'confirmed_ban'], inplace=True)\n",
    "print('x columns:\\n', x.columns)\n",
    "print('x head:\\n', x.head())\n",
    "print('y head:\\n', y.head())\n",
    "x.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This logic is roughly equivalent to app.py's train function\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "binary_classifier.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_classifier.score(test_y, test_x))\n",
    "# OUTPUT: (0.9992126580557206, 0.9992126580557205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=test_y, \n",
    "        y_pred=binary_classifier.predict(test_x)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime(\"2010/11/12\", format=\"%Y/%m/%d\")\n",
    "# Out[52]: Timestamp('2010-11-12 00:00:00')\n",
    "\n",
    "# pd.to_datetime(\"12-11-2010 00:00\", format=\"%d-%m-%Y %H:%M\")\n",
    "# Out[53]: Timestamp('2010-11-12 00:00:00')\n",
    "\n",
    "# TODO: play around with updated_at/created_at and other feature extraction\n",
    "# # print(features_labeled['created_at'].dtypes)\n",
    "# # created_at_ts = pd.to_datetime(features_labeled['created_at'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# # updated_at_ts = pd.to_datetime(features_labeled['updated_at'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# # acct_lifespan_ts = updated_at_ts - created_at_ts\n",
    "# # print(created_at_ts.head())\n",
    "# # print(updated_at_ts.head())\n",
    "# # print(acct_lifespan_ts)\n",
    "\n",
    "# created_at_ts.apply(lambda x: (x - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(\"1s\")).head()\n",
    "# (stamps - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "# pd.to_datetime(features_labeled['created_at'], format=\"%Y-%m-%d %H:%M:%S\").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca code loosely based on:\n",
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "\n",
    "def do_pca(X, standardize=True, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    if standardize:\n",
    "        estimator = make_pipeline(StandardScaler(), pca).fit(X)\n",
    "        X_r = estimator.transform(X)\n",
    "    else:\n",
    "        X_r = pca.fit(X).transform(X)\n",
    "    print(\"explained variance ratio: %s\" % str(pca.explained_variance_ratio_))\n",
    "    return X_r\n",
    "    \n",
    "def plot_2d_pca(X, y, omit_real=True, make_bad_plot=False, standardize=True):\n",
    "    y = y.copy()\n",
    "    y = pd.DataFrame(y)\n",
    "    print(y.head())\n",
    "    X_r = do_pca(X, standardize)\n",
    "    \n",
    "    if omit_real:\n",
    "        X_r = X_r[y['label'] != 'Real_Player']\n",
    "        y = y[y['label'] != 'Real_Player']\n",
    "    \n",
    "    if make_bad_plot:\n",
    "        fig, ax = plt.subplots(figsize=(1,1), dpi=80)\n",
    "        lw = 1\n",
    "        #for color, i in zip(colors, [0, 1, 2]):\n",
    "        label_to_id = dict((label, label_id) for label_id, label in enumerate(y['label'].unique()))\n",
    "        ax.scatter(X_r[:,0], X_r[:,1], c=y['label'].apply(lambda l: label_to_id[l]), alpha=0.3, norm='log', lw=lw, label=y['label'])\n",
    "        for i, label in enumerate(y['label'].values):\n",
    "            ax.annotate(label, (X_r[i,0], X_r[i,1]), fontsize=4)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "        # ax.title(\"PCA of IRIS dataset\")\n",
    "    \n",
    "    \n",
    "do_pca(x, standardize=True, n_components=len(train_x.columns))\n",
    "plot_2d_pca(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10 ipynb kernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
